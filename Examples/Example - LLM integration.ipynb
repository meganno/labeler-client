{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061ed4b0",
   "metadata": {},
   "source": [
    "# LLM-human collaborative annotation \n",
    "This notebook illustrates the integration of Large Language Models (LLMs) into the Labeler framework. In this framework, LLMs serve as annotators, and human verification is used to validate the annotation results. Initially, we demonstrate this integration with OpenAI's GPT models and completion APIs.\n",
    "\n",
    "Users can register agents by specifying model configurations and prompt configurations, select a subset, and run the job. Labeler takes care of the following tasks:\n",
    "\n",
    "* Interfacing with OpenAI and handling errors.\n",
    "* Executing LLM models and persisting the results.\n",
    "* Providing flexible search capabilities to support human verification and downstream applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48743562",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "## 1.1 Authentication and Labeler project connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb84fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeler_client import Authentication\n",
    "auth = Authentication(project=\"llm_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de013990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeler_client import Service\n",
    "service = Service(project='llm_demo', auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96725cc",
   "metadata": {},
   "source": [
    "## 1.2 Data import and schema config \n",
    "(For new project only, skip if already set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (skip) import data and create schema\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv('tweets.csv')\n",
    "# service.import_data_df(df, column_mapping={\n",
    "#     'id':'id',\n",
    "#     'content':'content'\n",
    "# })\n",
    "\n",
    "# service.get_schemas().set_schemas({\n",
    "#     'label_schema': [\n",
    "#         {\n",
    "#             \"name\": \"sentiment\",\n",
    "#             \"level\": \"record\",\n",
    "#             \"options\": [\n",
    "#                 { \"value\": \"pos\", \"text\": \"positive\" },\n",
    "#                 { \"value\": \"neu\", \"text\": \"neutral\" },\n",
    "#                 { \"value\": \"neg\", \"text\": \"negative\" },\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3f29f",
   "metadata": {},
   "source": [
    "## 1.3 Review labeling schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### review schema\n",
    "schema = service.get_schemas().value(active=True)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055afb69",
   "metadata": {},
   "source": [
    "# 2. LLM Annotation\n",
    "## 2.1 Config model and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'model': 'text-davinci-003', 'temperature': 0, 'n': 1} # define model configs\n",
    "# OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] # provide your open ai api key here\n",
    "# OPENAI_ORGANIZATION = os.environ['OPENAI_ORGANIZATION'] if 'OPENAI_ORGANIZATION' in os.environ else '' # provide an openai organization key if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760f126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from labeler_client.prompt import PromptTemplate\n",
    "prompt_template = PromptTemplate(label_schema=schema[0]['schemas']['label_schema'], label_names=[label_name])\n",
    "prompt_template.preview(records=['[sample input]', 'Megagon Labs is located in Mountain View.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.get_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12774b",
   "metadata": {},
   "source": [
    "## 2.2 Register an agent with service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from labeler_client.controller import Controller\n",
    "controller = Controller(service, auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6686c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_uuid = controller.create_agent(model_config, prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece3a62",
   "metadata": {},
   "source": [
    "## 2.3 Run an LLM annotation job on subsets\n",
    "**!Make sure OPENAI_API_KEY is set as an env var.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure OPENAI_API_KEY is set, be carefule not to commit the key\n",
    "import os\n",
    "#print(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062173f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset = service.search(keyword=\"delay\", limit=10, start=0)\n",
    "# subset.show({'view': 'table'}) # for data visualization\n",
    "job_uuid = controller.run_job(agent_uuid, subset, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba783d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: second job with same agent, different subset\n",
    "subset2 = service.search(keyword='great', limit=10, start=0)\n",
    "job_uuid2 = controller.run_job(agent_uuid, subset2, label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d8835",
   "metadata": {},
   "source": [
    "## 2.4 List agents & jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd3c49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agents = controller.list_my_agents()\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = controller.get_agent_by_uuid(agent_uuid)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8edef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controller.list_jobs_of_agent(agent_uuid)\n",
    "job_list = controller.list_jobs('agent_uuid', [agent_uuid])\n",
    "# controller.list_jobs('issued_by', [])\n",
    "job_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138124d2",
   "metadata": {},
   "source": [
    "# 3. Human Verification\n",
    "## 3.1 prepare for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, set to first job in the list from the previous query\n",
    "# replace with job_id to verify\n",
    "job_id = job_list[0][0] \n",
    "# Optional verified filter to retrieve a subset with or without verifications\n",
    "verf_subset = service.search_by_job(job_id=job_id, verified=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56486f69",
   "metadata": {},
   "source": [
    "## 3.2 Verify LLM output\n",
    "In the 'sentiment' column, the labels are generated by the LLM. If any of these labels need correction, simply update them in the user interface. For accurate labels, leave them unmodified. Once you have verified the subset, select all and click the 'Verify' button in the upper-left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "verf_subset.show({\"title\": \"Verification\", \"view\": \"table\", \"mode\": \"verifying\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef193897",
   "metadata": {},
   "source": [
    "## 3.3 Retrieve Verification Annotations\n",
    "The current version supports only programmatic retrieval of previous verifications, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23518178",
   "metadata": {},
   "outputs": [],
   "source": [
    "verf_subset.get_verification_annotations(\n",
    "    label_name=\"sentiment\", \n",
    "    label_level=\"record\",\n",
    "    annotator=job_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6374ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further filter by type of verification(CONFIRMS|CORRECTS)\n",
    "# CONFIMS:  where the verification confirms the original label\n",
    "# CORRECTS: where the verification is different from the original label\n",
    "verf_subset.get_verification_annotations(\n",
    "    label_name=\"sentiment\", \n",
    "    label_level=\"record\",\n",
    "    annotator=job_id,\n",
    "    verified_status='CORRECTS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc764fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
